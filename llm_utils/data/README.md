# Data Utilities

This directory contains scripts and utilities for preparing and analyzing data for Large Language Models.

## Core Components

- **`analyze_token_lengths.py`**: A script to analyze the token lengths of a dataset, which can be useful for determining appropriate `max_input_length` values for model training.

- **`dataset_loading.py`**: Provides functions for loading and processing various dataset formats.

- **`intersect_dataset.py`**: A utility to find the intersection of two datasets based on common identifiers.

- **`semantic_sample.py`**: A script to draw a semantically diverse subset from a larger dataset, useful for creating representative evaluation sets.

## Usage

For detailed usage examples, please refer to the main `README.md` file in the root of the repository.
